---
title: "Assignment 01"
output: github_document
---

## Big Data Definition

Big Data refers to extremely large and complex data sets that are difficult to process using traditional data processing tools and techniques. The volume, velocity, and variety of big data make it challenging to store, manage, and analyze.

The goal of big data is to extract valuable insights and knowledge from the data to support informed decision-making and drive innovation. This can include understanding customer behavior, improving operational efficiency, predicting future trends, and developing new products and services.

In short, big data refers to extremely large and complex data sets that require new technologies and approaches to extract value and drive innovation.

## 6 V's of Big Data

The "6 V's of Big Data" refers to the six characteristics that define big data, which are as follows:

1. **Volume**: The amount of data generated, which can be in petabytes or even exabytes.   

2. **Velocity**: The speed at which the data is generated and processed, which can be in real-time. For example, social media data or sensor data.

3. **Variety**: The different types of data generated, which can include structured data (such as numbers and dates), semi-structured data (such as text and images), and unstructured data (such as audio and video).

4. **Veracity**: The uncertainty, ambiguity, and incompleteness of the data, which can impact its accuracy and usefulness. For example, sensor data can be inconsitant

5. **Value**: The potential business value that can be derived from the data, such as improved decision-making, enhanced customer experience, and increased operational efficiency.

6. **Variability**: The change in data characteristics over time, such as the format, structure, and content of the data.

## Phases of Big Data analysis

The phases of big data analysis can be grouped into five main stages, which are:

1. **Data Acquisition and Recording** : This stage involves gathering data from various sources, such as databases, log files, social media, sensors, and other sources. The data collected in this stage needs to be cleaned, organized, and transformed into a format that can be analyzed.

2. **Information Extraction and Cleaning** :  Information extraction and data cleaning are crucial steps in the big data process, as they help to ensure that the data is accurate, consistent, and usable for analysis. By taking the time to properly extract and clean the data, businesses can gain valuable insights that can help to drive growth and improve decision-making.

3. **Data Integration, Aggregation, and Representation** : This stage involves merging data from multiple sources. The summarized aggregated data can help leaders make well-informed decisions. Data Representation refers to visualisation of the data. Many data visualization techniques could be used for representation: Bar chart, Histogram, Boxplot, Pie chart.

4. **Query Processing, Data Modeling, and Analysis** : Querying can be complex due to the sheer volume of data and the need to distribute processing across multiple nodes in a cluster. Techniques such as distributed query processing, indexing, and caching are commonly used. Data Modeling is the process of designing the structure of a database or data warehouse. Common approaches to data modeling in big data include schema-on-read, NoSQL databases, and data lakes. In big data, analysis is typically performed using advanced techniques such as machine learning, data mining, and statistical analysis.

5. **Interpretation** : The interpretation phase in big data refers to the process of analyzing and making sense of the insights and knowledge extracted from large and complex datasets. This phase is a critical component of big data analytics, as it allows organizations to use the insights they have gained to inform their decision-making processes.

## Challenges in Big Data analysis

- **Heterogeneity and Incompleteness** : Handling large volume of generated data can be challenging because it will be of different data types and might include missing data.

- **Scale**: The term "scale" describes the enormous amount of data that must be processed, stored, and analyzed. The scale of big data can be so large that typical data processing tools may not be able to manage it effectively.

- **Timeliness**: Big data's "timeliness challenge" refers to the requirement for real-time or almost real-time data processing and analysis.

- **Privacy** : The collection and analysis of large amounts of information can result in potential privacy violations and breaches, making privacy a significant challenge in big data.

- **Human Collaboration**: Establishing efficient communication and collaboration processes between various teams and stakeholders is referred to as the human collaboration challenge in big data. This is especially difficult when dealing with large amounts of data that require specialized knowledge and skills to manage and analyze.


